{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_name = \"roberta-large-mnli\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight torch.Size([50265, 1024])\n",
      "roberta.embeddings.position_embeddings.weight torch.Size([514, 1024])\n",
      "roberta.embeddings.token_type_embeddings.weight torch.Size([1, 1024])\n",
      "roberta.embeddings.LayerNorm.weight torch.Size([1024])\n",
      "roberta.embeddings.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.0.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.0.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.0.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.0.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.0.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.0.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.0.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.0.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.0.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.0.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.0.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.0.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.1.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.1.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.1.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.1.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.1.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.1.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.1.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.1.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.1.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.1.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.1.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.1.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.2.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.2.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.2.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.2.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.2.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.2.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.2.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.2.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.2.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.2.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.2.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.2.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.3.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.3.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.3.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.3.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.3.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.3.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.3.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.3.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.3.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.3.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.3.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.3.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.4.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.4.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.4.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.4.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.4.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.4.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.4.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.4.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.4.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.4.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.4.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.4.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.5.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.5.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.5.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.5.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.5.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.5.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.5.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.5.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.5.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.5.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.5.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.5.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.6.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.6.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.6.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.6.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.6.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.6.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.6.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.6.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.6.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.6.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.6.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.6.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.7.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.7.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.7.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.7.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.7.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.7.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.7.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.7.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.7.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.7.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.7.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.7.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.8.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.8.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.8.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.8.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.8.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.8.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.8.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.8.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.8.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.8.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.8.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.8.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.9.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.9.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.9.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.9.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.9.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.9.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.9.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.9.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.9.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.9.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.9.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.9.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.10.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.10.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.10.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.10.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.10.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.10.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.10.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.10.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.10.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.10.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.10.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.10.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.11.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.11.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.11.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.11.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.11.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.11.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.11.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.11.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.11.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.11.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.11.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.11.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.12.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.12.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.12.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.12.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.12.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.12.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.12.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.12.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.12.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.12.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.12.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.12.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.12.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.12.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.12.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.12.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.13.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.13.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.13.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.13.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.13.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.13.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.13.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.13.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.13.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.13.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.13.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.13.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.13.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.13.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.13.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.13.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.14.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.14.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.14.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.14.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.14.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.14.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.14.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.14.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.14.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.14.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.14.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.14.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.14.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.14.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.14.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.14.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.15.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.15.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.15.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.15.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.15.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.15.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.15.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.15.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.15.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.15.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.15.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.15.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.15.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.15.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.15.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.15.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.16.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.16.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.16.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.16.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.16.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.16.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.16.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.16.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.16.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.16.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.16.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.16.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.16.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.16.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.16.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.16.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.17.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.17.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.17.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.17.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.17.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.17.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.17.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.17.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.17.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.17.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.17.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.17.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.17.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.17.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.17.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.17.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.18.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.18.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.18.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.18.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.18.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.18.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.18.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.18.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.18.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.18.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.18.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.18.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.18.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.18.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.18.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.18.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.19.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.19.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.19.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.19.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.19.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.19.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.19.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.19.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.19.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.19.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.19.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.19.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.19.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.19.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.19.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.19.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.20.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.20.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.20.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.20.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.20.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.20.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.20.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.20.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.20.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.20.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.20.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.20.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.20.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.20.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.20.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.20.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.21.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.21.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.21.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.21.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.21.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.21.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.21.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.21.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.21.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.21.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.21.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.21.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.21.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.21.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.21.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.21.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.22.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.22.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.22.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.22.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.22.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.22.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.22.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.22.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.22.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.22.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.22.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.22.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.22.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.22.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.22.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.22.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.23.attention.self.query.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.23.attention.self.query.bias torch.Size([1024])\n",
      "roberta.encoder.layer.23.attention.self.key.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.23.attention.self.key.bias torch.Size([1024])\n",
      "roberta.encoder.layer.23.attention.self.value.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.23.attention.self.value.bias torch.Size([1024])\n",
      "roberta.encoder.layer.23.attention.output.dense.weight torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.23.attention.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.23.attention.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.23.attention.output.LayerNorm.bias torch.Size([1024])\n",
      "roberta.encoder.layer.23.intermediate.dense.weight torch.Size([4096, 1024])\n",
      "roberta.encoder.layer.23.intermediate.dense.bias torch.Size([4096])\n",
      "roberta.encoder.layer.23.output.dense.weight torch.Size([1024, 4096])\n",
      "roberta.encoder.layer.23.output.dense.bias torch.Size([1024])\n",
      "roberta.encoder.layer.23.output.LayerNorm.weight torch.Size([1024])\n",
      "roberta.encoder.layer.23.output.LayerNorm.bias torch.Size([1024])\n",
      "classifier.dense.weight torch.Size([1024, 1024])\n",
      "classifier.dense.bias torch.Size([1024])\n",
      "classifier.out_proj.weight torch.Size([3, 1024])\n",
      "classifier.out_proj.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wangrongsheng/ag_news\")\n",
    "\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_built():\n",
    "#     device = torch.device(\"mps\")\n",
    "# else:\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 7600\n",
      "})\n",
      "{'text': \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\", 'label': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'badhiyab hai badhiya',\n",
       " 'labels': ['cooking',\n",
       "  'dancing',\n",
       "  'travel',\n",
       "  'sports person',\n",
       "  'actor',\n",
       "  'footballer',\n",
       "  'soccer player',\n",
       "  'cricket'],\n",
       " 'scores': [0.2231716364622116,\n",
       "  0.16547532379627228,\n",
       "  0.1618598997592926,\n",
       "  0.13351215422153473,\n",
       "  0.10129763185977936,\n",
       "  0.07446768134832382,\n",
       "  0.07138216495513916,\n",
       "  0.06883353739976883]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds[\"test\"])\n",
    "print(ds[\"test\"][0])\n",
    "classifier = pipeline('zero-shot-classification', model='roberta-large-mnli', device=0)                              \n",
    "sequence_to_classify = \"badhiyab hai badhiya\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing', 'cricket', 'sports person', 'footballer', 'soccer player', 'actor']\n",
    "classifier(sequence_to_classify, candidate_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def perform_error_analysis(dataset, classifier, num_samples=100):\n",
    "    \"\"\"\n",
    "    Perform comprehensive error analysis on the zero-shot classifier.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The AG News dataset\n",
    "        classifier: The zero-shot classification pipeline\n",
    "        num_samples: Number of samples to analyze (use smaller number for testing)\n",
    "    \"\"\"\n",
    "    # AG News labels mapping\n",
    "    label_map = {\n",
    "        0: \"This document is about politics\",\n",
    "        1: \"This document is about sports\",\n",
    "        2: \"This document is about economics\",\n",
    "        3: \"This document is about science and technology\"\n",
    "    }\n",
    "    \n",
    "    # Prepare candidate labels for zero-shot classification\n",
    "    candidate_labels = list(label_map.values())\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Process test samples\n",
    "    for i, item in tqdm(enumerate(dataset['test']), total=num_samples):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        text = item['text']\n",
    "        true_label = label_map[item['label']]\n",
    "        \n",
    "        # Get model prediction\n",
    "        prediction = classifier(text, candidate_labels)\n",
    "        predicted_label = prediction['labels'][0]\n",
    "        confidence = prediction['scores'][0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'confidence': confidence,\n",
    "            'correct': true_label == predicted_label\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # 1. Overall Accuracy\n",
    "    accuracy = (df_results['correct'].sum() / len(df_results)) * 100\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # 2. Per-class Performance\n",
    "    print(\"\\nPer-class Performance:\")\n",
    "    print(classification_report(df_results['true_label'], df_results['predicted_label']))\n",
    "    \n",
    "    # 3. Confusion Matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(df_results['true_label'], df_results['predicted_label'], labels=candidate_labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=candidate_labels, yticklabels=candidate_labels)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Confidence Analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='true_label', y='confidence', hue='correct', data=df_results)\n",
    "    plt.title('Confidence Distribution by Class and Correctness')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Error Examples Analysis\n",
    "    print(\"\\nMost Confident Mistakes:\")\n",
    "    mistakes = df_results[~df_results['correct']].sort_values('confidence', ascending=False)\n",
    "    print(mistakes[['text', 'true_label', 'predicted_label', 'confidence']].head())\n",
    "    \n",
    "    # 6. Length Analysis\n",
    "    df_results['text_length'] = df_results['text'].str.len()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='true_label', y='text_length', hue='correct', data=df_results)\n",
    "    plt.title('Text Length Distribution by Class and Correctness')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_patterns(df_results):\n",
    "    \"\"\"\n",
    "    Analyze specific patterns in the errors.\n",
    "    \"\"\"\n",
    "    # Common misclassification patterns\n",
    "    error_patterns = defaultdict(int)\n",
    "    for _, row in df_results[~df_results['correct']].iterrows():\n",
    "        pattern = f\"{row['true_label']} â†’ {row['predicted_label']}\"\n",
    "        error_patterns[pattern] += 1\n",
    "    \n",
    "    print(\"\\nCommon Error Patterns:\")\n",
    "    for pattern, count in sorted(error_patterns.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{pattern}: {count}\")\n",
    "        \n",
    "    # Confidence threshold analysis\n",
    "    thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "    accuracies = []\n",
    "    coverage = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        filtered_preds = df_results[df_results['confidence'] >= threshold]\n",
    "        if len(filtered_preds) > 0:\n",
    "            acc = (filtered_preds['correct'].sum() / len(filtered_preds)) * 100\n",
    "            cov = (len(filtered_preds) / len(df_results)) * 100\n",
    "            accuracies.append(acc)\n",
    "            coverage.append(cov)\n",
    "    \n",
    "    # Ensure arrays are the same length before plotting\n",
    "    min_len = min(len(accuracies), len(coverage))\n",
    "    thresholds = thresholds[:min_len]\n",
    "    accuracies = accuracies[:min_len]\n",
    "    coverage = coverage[:min_len]\n",
    "    \n",
    "    if min_len > 0:  # Only plot if we have data\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(thresholds, accuracies, 'b-', label='Accuracy')\n",
    "        plt.plot(thresholds, coverage, 'r-', label='Coverage')\n",
    "        plt.xlabel('Confidence Threshold')\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.title('Accuracy vs Coverage Trade-off')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Warning: Not enough data points to create accuracy-coverage plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b64f62d6714ce283faef6c588a257a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Perform main error analysis\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     results_df \u001b[38;5;241m=\u001b[39m \u001b[43mperform_error_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Analyze error patterns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     analyze_error_patterns(results_df)\n",
      "Cell \u001b[0;32mIn[138], line 41\u001b[0m, in \u001b[0;36mperform_error_analysis\u001b[0;34m(dataset, classifier, num_samples)\u001b[0m\n\u001b[1;32m     38\u001b[0m true_label \u001b[38;5;241m=\u001b[39m label_map[item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Get model prediction\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m prediction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     43\u001b[0m confidence \u001b[38;5;241m=\u001b[39m prediction[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/zero_shot_classification.py:206\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/base.py:1260\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/base.py:1175\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1174\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1175\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/zero_shot_classification.py:229\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    228\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: candidate_label,\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sequence,\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutputs,\n\u001b[1;32m    236\u001b[0m }\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1339\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1339\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1351\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:976\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    974\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 976\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    989\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    622\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m         output_attentions,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:562\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    559\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    560\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 562\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:575\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    574\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 575\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:486\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 486\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    488\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Perform main error analysis\n",
    "    results_df = perform_error_analysis(ds, classifier, num_samples=500)\n",
    "    \n",
    "    # Analyze error patterns\n",
    "    analyze_error_patterns(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a231d61800dd491ab37c18ba7f8c0697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e117510632b147a49783ae715ac6eb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00017.safetensors:  39%|###9      | 1.53G/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c5224b4bc74154b2baf39007cbe175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee0bca282f741dda190036809cb6106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00017.safetensors:  40%|###9      | 1.55G/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126805dfd1f244efa05e821b18bee600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00017.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcad7d7454574a2d90a1a58a893682dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00017.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19eb118f6dc04bf384a79699b715878b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00017.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/e3/79/e3796211fc025913b53a6b38d22e1cbcc7bb19c33761017c256532b8d0e1bc3d/47f562bf77d986b53c5390b01a30a77576c3eed69191b484cf88b53bb64bce19?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00009-of-00017.safetensors%3B+filename%3D%22model-00009-of-00017.safetensors%22%3B&Expires=1731932779&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTkzMjc3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2UzLzc5L2UzNzk2MjExZmMwMjU5MTNiNTNhNmIzOGQyMmUxY2JjYzdiYjE5YzMzNzYxMDE3YzI1NjUzMmI4ZDBlMWJjM2QvNDdmNTYyYmY3N2Q5ODZiNTNjNTM5MGIwMWEzMGE3NzU3NmMzZWVkNjkxOTFiNDg0Y2Y4OGI1M2JiNjRiY2UxOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=A8pJGohbCVm9-NFbyy7-MTdcW4xBd3nQvGFd7KH1k5ouHfWnP8XoBG6WrHDTDkrYt8G6h0-hxUzjL2g7K4UpOtWHc2qwyWlBgkXJHx-Qp10UuPXbh0-WuNSkzpwK8FgyppAyDjpss5OXo5ThrnJIE4vER7SLf000RVIMbyjIPcLl8ctLRmb4%7EFW9yQJ%7EyY97%7EJYGjohmrlrEEF8262XmmsD9Uz8VfWgkH4z6k-g472Yq3WWzeuPKGNLHkssmd1D238pZTarOIwklEGz9bq5rHTgJoSi5HIZ56dtLKxN4a5y561d5SZKzW2XhCxfXyN7coBJI1%7EEMXnKCOfeD51oYyg__&Key-Pair-Id=K24J24Z295AEI9: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model Qwen/Qwen2.5-32B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 748, in _error_catcher\n    yield\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 894, in _raw_read\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\nurllib3.exceptions.IncompleteRead: IncompleteRead(27048283 bytes read, 2342878301 more expected)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/models.py\", line 820, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 1060, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 977, in read\n    data = self._raw_read(amt)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 872, in _raw_read\n    with self._error_catcher():\n  File \"/root/anaconda3/envs/nst/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 772, in _error_catcher\n    raise ProtocolError(arg, e) from e\nurllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(27048283 bytes read, 2342878301 more expected)', IncompleteRead(27048283 bytes read, 2342878301 more expected))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3769, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/utils/hub.py\", line 1098, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 549, in http_get\n    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/models.py\", line 822, in generate\n    raise ChunkedEncodingError(e)\nrequests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(27048283 bytes read, 2342878301 more expected)', IncompleteRead(27048283 bytes read, 2342878301 more expected))\n\nwhile loading with Qwen2ForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 748, in _error_catcher\n    yield\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 873, in _raw_read\n    data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 856, in _fp_read\n    return self._fp.read(amt) if amt is not None else self._fp.read()\n  File \"/root/anaconda3/envs/nst/lib/python3.10/http/client.py\", line 466, in read\n    s = self.fp.read(amt)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/socket.py\", line 717, in readinto\n    return self._sock.recv_into(b)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/ssl.py\", line 1307, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/ssl.py\", line 1163, in read\n    return self._sslobj.read(len, buffer)\nTimeoutError: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/models.py\", line 820, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 1060, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 949, in read\n    data = self._raw_read(amt)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 872, in _raw_read\n    with self._error_catcher():\n  File \"/root/anaconda3/envs/nst/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 753, in _error_catcher\n    raise ReadTimeoutError(self._pool, None, \"Read timed out.\") from e  # type: ignore[arg-type]\nurllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 549, in http_get\n    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/models.py\", line 826, in generate\n    raise ConnectionError(e)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connection.py\", line 730, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/root/anaconda3/envs/nst/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\nurllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n    retries = retries.increment(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/util/retry.py\", line 519, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Max retries exceeded with url: /repos/e3/79/e3796211fc025913b53a6b38d22e1cbcc7bb19c33761017c256532b8d0e1bc3d/47f562bf77d986b53c5390b01a30a77576c3eed69191b484cf88b53bb64bce19?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00009-of-00017.safetensors%3B+filename%3D%22model-00009-of-00017.safetensors%22%3B&Expires=1731932779&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTkzMjc3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2UzLzc5L2UzNzk2MjExZmMwMjU5MTNiNTNhNmIzOGQyMmUxY2JjYzdiYjE5YzMzNzYxMDE3YzI1NjUzMmI4ZDBlMWJjM2QvNDdmNTYyYmY3N2Q5ODZiNTNjNTM5MGIwMWEzMGE3NzU3NmMzZWVkNjkxOTFiNDg0Y2Y4OGI1M2JiNjRiY2UxOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=A8pJGohbCVm9-NFbyy7-MTdcW4xBd3nQvGFd7KH1k5ouHfWnP8XoBG6WrHDTDkrYt8G6h0-hxUzjL2g7K4UpOtWHc2qwyWlBgkXJHx-Qp10UuPXbh0-WuNSkzpwK8FgyppAyDjpss5OXo5ThrnJIE4vER7SLf000RVIMbyjIPcLl8ctLRmb4~FW9yQJ~yY97~JYGjohmrlrEEF8262XmmsD9Uz8VfWgkH4z6k-g472Yq3WWzeuPKGNLHkssmd1D238pZTarOIwklEGz9bq5rHTgJoSi5HIZ56dtLKxN4a5y561d5SZKzW2XhCxfXyN7coBJI1~EMXnKCOfeD51oYyg__&Key-Pair-Id=K24J24Z295AEI9 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3769, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/utils/hub.py\", line 1098, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 566, in http_get\n    return http_get(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 463, in http_get\n    r = _request_wrapper(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 399, in _request_wrapper\n    response = get_session().request(method=method, url=url, **params)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 66, in send\n    return super().send(request, *args, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/adapters.py\", line 698, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: (MaxRetryError(\"HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Max retries exceeded with url: /repos/e3/79/e3796211fc025913b53a6b38d22e1cbcc7bb19c33761017c256532b8d0e1bc3d/47f562bf77d986b53c5390b01a30a77576c3eed69191b484cf88b53bb64bce19?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00009-of-00017.safetensors%3B+filename%3D%22model-00009-of-00017.safetensors%22%3B&Expires=1731932779&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTkzMjc3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2UzLzc5L2UzNzk2MjExZmMwMjU5MTNiNTNhNmIzOGQyMmUxY2JjYzdiYjE5YzMzNzYxMDE3YzI1NjUzMmI4ZDBlMWJjM2QvNDdmNTYyYmY3N2Q5ODZiNTNjNTM5MGIwMWEzMGE3NzU3NmMzZWVkNjkxOTFiNDg0Y2Y4OGI1M2JiNjRiY2UxOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=A8pJGohbCVm9-NFbyy7-MTdcW4xBd3nQvGFd7KH1k5ouHfWnP8XoBG6WrHDTDkrYt8G6h0-hxUzjL2g7K4UpOtWHc2qwyWlBgkXJHx-Qp10UuPXbh0-WuNSkzpwK8FgyppAyDjpss5OXo5ThrnJIE4vER7SLf000RVIMbyjIPcLl8ctLRmb4~FW9yQJ~yY97~JYGjohmrlrEEF8262XmmsD9Uz8VfWgkH4z6k-g472Yq3WWzeuPKGNLHkssmd1D238pZTarOIwklEGz9bq5rHTgJoSi5HIZ56dtLKxN4a5y561d5SZKzW2XhCxfXyN7coBJI1~EMXnKCOfeD51oYyg__&Key-Pair-Id=K24J24Z295AEI9 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)')))\"), '(Request ID: 8235f8c4-7eab-40e5-a44c-7491fc2bf524)')\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      4\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQwen/Qwen2.5-32B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m pipe(messages)\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/__init__.py:896\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 896\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    907\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/base.py:301\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    300\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model Qwen/Qwen2.5-32B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 748, in _error_catcher\n    yield\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 894, in _raw_read\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\nurllib3.exceptions.IncompleteRead: IncompleteRead(27048283 bytes read, 2342878301 more expected)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/models.py\", line 820, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 1060, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 977, in read\n    data = self._raw_read(amt)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 872, in _raw_read\n    with self._error_catcher():\n  File \"/root/anaconda3/envs/nst/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 772, in _error_catcher\n    raise ProtocolError(arg, e) from e\nurllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(27048283 bytes read, 2342878301 more expected)', IncompleteRead(27048283 bytes read, 2342878301 more expected))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3769, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/utils/hub.py\", line 1098, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 549, in http_get\n    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/models.py\", line 822, in generate\n    raise ChunkedEncodingError(e)\nrequests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(27048283 bytes read, 2342878301 more expected)', IncompleteRead(27048283 bytes read, 2342878301 more expected))\n\nwhile loading with Qwen2ForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 748, in _error_catcher\n    yield\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 873, in _raw_read\n    data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 856, in _fp_read\n    return self._fp.read(amt) if amt is not None else self._fp.read()\n  File \"/root/anaconda3/envs/nst/lib/python3.10/http/client.py\", line 466, in read\n    s = self.fp.read(amt)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/socket.py\", line 717, in readinto\n    return self._sock.recv_into(b)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/ssl.py\", line 1307, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/ssl.py\", line 1163, in read\n    return self._sslobj.read(len, buffer)\nTimeoutError: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/models.py\", line 820, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 1060, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 949, in read\n    data = self._raw_read(amt)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 872, in _raw_read\n    with self._error_catcher():\n  File \"/root/anaconda3/envs/nst/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/response.py\", line 753, in _error_catcher\n    raise ReadTimeoutError(self._pool, None, \"Read timed out.\") from e  # type: ignore[arg-type]\nurllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 549, in http_get\n    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/models.py\", line 826, in generate\n    raise ConnectionError(e)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connection.py\", line 730, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connection.py\", line 909, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/util/ssl_.py\", line 469, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/util/ssl_.py\", line 513, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/root/anaconda3/envs/nst/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\nurllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n    retries = retries.increment(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/urllib3/util/retry.py\", line 519, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Max retries exceeded with url: /repos/e3/79/e3796211fc025913b53a6b38d22e1cbcc7bb19c33761017c256532b8d0e1bc3d/47f562bf77d986b53c5390b01a30a77576c3eed69191b484cf88b53bb64bce19?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00009-of-00017.safetensors%3B+filename%3D%22model-00009-of-00017.safetensors%22%3B&Expires=1731932779&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTkzMjc3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2UzLzc5L2UzNzk2MjExZmMwMjU5MTNiNTNhNmIzOGQyMmUxY2JjYzdiYjE5YzMzNzYxMDE3YzI1NjUzMmI4ZDBlMWJjM2QvNDdmNTYyYmY3N2Q5ODZiNTNjNTM5MGIwMWEzMGE3NzU3NmMzZWVkNjkxOTFiNDg0Y2Y4OGI1M2JiNjRiY2UxOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=A8pJGohbCVm9-NFbyy7-MTdcW4xBd3nQvGFd7KH1k5ouHfWnP8XoBG6WrHDTDkrYt8G6h0-hxUzjL2g7K4UpOtWHc2qwyWlBgkXJHx-Qp10UuPXbh0-WuNSkzpwK8FgyppAyDjpss5OXo5ThrnJIE4vER7SLf000RVIMbyjIPcLl8ctLRmb4~FW9yQJ~yY97~JYGjohmrlrEEF8262XmmsD9Uz8VfWgkH4z6k-g472Yq3WWzeuPKGNLHkssmd1D238pZTarOIwklEGz9bq5rHTgJoSi5HIZ56dtLKxN4a5y561d5SZKzW2XhCxfXyN7coBJI1~EMXnKCOfeD51oYyg__&Key-Pair-Id=K24J24Z295AEI9 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 288, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3769, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/utils/hub.py\", line 1098, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 566, in http_get\n    return http_get(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 463, in http_get\n    r = _request_wrapper(\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 399, in _request_wrapper\n    response = get_session().request(method=method, url=url, **params)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 66, in send\n    return super().send(request, *args, **kwargs)\n  File \"/root/anaconda3/envs/nst/lib/python3.10/site-packages/requests/adapters.py\", line 698, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: (MaxRetryError(\"HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Max retries exceeded with url: /repos/e3/79/e3796211fc025913b53a6b38d22e1cbcc7bb19c33761017c256532b8d0e1bc3d/47f562bf77d986b53c5390b01a30a77576c3eed69191b484cf88b53bb64bce19?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00009-of-00017.safetensors%3B+filename%3D%22model-00009-of-00017.safetensors%22%3B&Expires=1731932779&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTkzMjc3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2UzLzc5L2UzNzk2MjExZmMwMjU5MTNiNTNhNmIzOGQyMmUxY2JjYzdiYjE5YzMzNzYxMDE3YzI1NjUzMmI4ZDBlMWJjM2QvNDdmNTYyYmY3N2Q5ODZiNTNjNTM5MGIwMWEzMGE3NzU3NmMzZWVkNjkxOTFiNDg0Y2Y4OGI1M2JiNjRiY2UxOT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=A8pJGohbCVm9-NFbyy7-MTdcW4xBd3nQvGFd7KH1k5ouHfWnP8XoBG6WrHDTDkrYt8G6h0-hxUzjL2g7K4UpOtWHc2qwyWlBgkXJHx-Qp10UuPXbh0-WuNSkzpwK8FgyppAyDjpss5OXo5ThrnJIE4vER7SLf000RVIMbyjIPcLl8ctLRmb4~FW9yQJ~yY97~JYGjohmrlrEEF8262XmmsD9Uz8VfWgkH4z6k-g472Yq3WWzeuPKGNLHkssmd1D238pZTarOIwklEGz9bq5rHTgJoSi5HIZ56dtLKxN4a5y561d5SZKzW2XhCxfXyN7coBJI1~EMXnKCOfeD51oYyg__&Key-Pair-Id=K24J24Z295AEI9 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)')))\"), '(Request ID: 8235f8c4-7eab-40e5-a44c-7491fc2bf524)')\n\n\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-32B-Instruct\")\n",
    "pipe(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
