{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "import subprocess\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_name = \"roberta-large-mnli\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"wangrongsheng/ag_news\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def start_gemma2_session():\n",
    "#     \"\"\"\n",
    "#     Start Gemma2 through Ollama and return the process instance.\n",
    "\n",
    "#     Returns:\n",
    "#         subprocess.Popen: Persistent Gemma2 process.\n",
    "#     \"\"\"\n",
    "#     gemma2_process = subprocess.Popen(\n",
    "#         [\"ollama\", \"run\", \"gemma2\"],\n",
    "#         stdin=subprocess.PIPE,\n",
    "#         stdout=subprocess.PIPE,\n",
    "#         stderr=subprocess.PIPE,\n",
    "#         text=True\n",
    "#     )\n",
    "#     return gemma2_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def interact_with_gemma2(gemma2_process, prompt):\n",
    "#     \"\"\"\n",
    "#     Send a prompt to Gemma2 and retrieve its response.\n",
    "\n",
    "#     Args:\n",
    "#         gemma2_process (subprocess.Popen): The running Gemma2 process.\n",
    "#         prompt (str): The prompt to send.\n",
    "\n",
    "#     Returns:\n",
    "#         str: The response from Gemma2.\n",
    "#     \"\"\"\n",
    "#     # Write the prompt to Gemma2\n",
    "#     gemma2_process.stdin.write(prompt + \"\\n\")\n",
    "#     gemma2_process.stdin.flush()\n",
    "\n",
    "#     # Read the response\n",
    "#     response = gemma2_process.stdout.readline().strip()\n",
    "#     return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a test',\n",
       " 'labels': ['technician', 'travel', 'teacher', 'dancing', 'cooking'],\n",
       " 'scores': [0.43758732080459595,\n",
       "  0.1701793223619461,\n",
       "  0.1603546291589737,\n",
       "  0.13061030209064484,\n",
       "  0.10126844793558121]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline('zero-shot-classification', model=model_name, device=1)                              \n",
    "sequence_to_classify = \"This is a test\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing', 'technician', 'teacher']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_error_analysis(dataset, classifier, label_map, num_samples=100):\n",
    "    \"\"\"\n",
    "    Perform comprehensive error analysis on the zero-shot classifier.\n",
    "    Returns the analysis results as a string and the results DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The AG News dataset\n",
    "        classifier: The zero-shot classification pipeline\n",
    "        label_map: Dictionary mapping label indices to descriptions\n",
    "        num_samples: Number of samples to analyze (use smaller number for testing)\n",
    "    \"\"\"\n",
    "    output_string = \"\"\n",
    "    \n",
    "    # Prepare candidate labels for zero-shot classification\n",
    "    candidate_labels = list(label_map.values())\n",
    "    output_string += f\"Candidate labels: {candidate_labels}\\n\\n\"\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Process test samples\n",
    "    for i, item in tqdm(enumerate(dataset['test']), total=num_samples):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        text = item['text']\n",
    "        true_label = label_map[item['label']]\n",
    "        \n",
    "        # Get model prediction\n",
    "        prediction = classifier(text, candidate_labels)\n",
    "        predicted_label = prediction['labels'][0]\n",
    "        confidence = prediction['scores'][0]\n",
    "        \n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'confidence': confidence,\n",
    "            'correct': true_label == predicted_label\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # 1. Overall Accuracy\n",
    "    accuracy = (df_results['correct'].sum() / len(df_results)) * 100\n",
    "    output_string += f\"Overall Accuracy: {accuracy:.2f}%\\n\\n\"\n",
    "    \n",
    "    # 2. Per-class Performance\n",
    "    output_string += \"Per-class Performance:\\n\"\n",
    "    class_report = classification_report(df_results['true_label'], df_results['predicted_label'])\n",
    "    output_string += f\"{class_report}\\n\"\n",
    "    \n",
    "    # 5. Error Examples Analysis\n",
    "    output_string += \"Most Confident Mistakes:\\n\"\n",
    "    mistakes = df_results[~df_results['correct']].sort_values('confidence', ascending=False)\n",
    "    mistake_df = mistakes[['text', 'true_label', 'predicted_label', 'confidence']].head()\n",
    "    output_string += mistake_df.to_string()\n",
    "    output_string += \"\\n\"\n",
    "\n",
    "    return df_results, output_string, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def analyze_error_patterns(df_results):\n",
    "    \"\"\"\n",
    "    Analyze specific patterns in the errors and return them as a string.\n",
    "    \"\"\"\n",
    "    # Common misclassification patterns\n",
    "    error_patterns = defaultdict(int)\n",
    "    for _, row in df_results[~df_results['correct']].iterrows():\n",
    "        pattern = f\"{row['true_label']} â†’ {row['predicted_label']}\"\n",
    "        error_patterns[pattern] += 1\n",
    "\n",
    "    # Collect the output lines in a list\n",
    "    output_lines = [\"\\nCommon Error Patterns:\"]\n",
    "    for pattern, count in sorted(error_patterns.items(), key=lambda x: x[1], reverse=True):\n",
    "        output_lines.append(f\"{pattern}: {count}\")\n",
    "\n",
    "    # Join the lines into a single string and return\n",
    "    # print(\"\\n\".join(output_lines))\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "        \n",
    "    # # Confidence threshold analysis\n",
    "    # thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "    # accuracies = []\n",
    "    # coverage = []\n",
    "    \n",
    "    # for threshold in thresholds:\n",
    "    #     filtered_preds = df_results[df_results['confidence'] >= threshold]\n",
    "    #     if len(filtered_preds) > 0:\n",
    "    #         acc = (filtered_preds['correct'].sum() / len(filtered_preds)) * 100\n",
    "    #         cov = (len(filtered_preds) / len(df_results)) * 100\n",
    "    #         accuracies.append(acc)\n",
    "    #         coverage.append(cov)\n",
    "    \n",
    "    # # Ensure arrays are the same length before plotting\n",
    "    # min_len = min(len(accuracies), len(coverage))\n",
    "    # thresholds = thresholds[:min_len]\n",
    "    # accuracies = accuracies[:min_len]\n",
    "    # coverage = coverage[:min_len]\n",
    "    \n",
    "    # if min_len > 0:  # Only plot if we have data\n",
    "    #     plt.figure(figsize=(10, 6))\n",
    "    #     plt.plot(thresholds, accuracies, 'b-', label='Accuracy')\n",
    "    #     plt.plot(thresholds, coverage, 'r-', label='Coverage')\n",
    "    #     plt.xlabel('Confidence Threshold')\n",
    "    #     plt.ylabel('Percentage')\n",
    "    #     plt.title('Accuracy vs Coverage Trade-off')\n",
    "    #     plt.legend()\n",
    "    #     plt.grid(True)\n",
    "    #     plt.show()\n",
    "    # else:\n",
    "    #     print(\"Warning: Not enough data points to create accuracy-coverage plot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize Gemma2 session\n",
    "#     gemma2 = start_gemma2_session()\n",
    "\n",
    "#     # Initial prompt\n",
    "#     initial_prompt = \"You are an expert prompt generator for a zero shot text classification task. I am using roberta-large-mnli to do zero shot classification on the ag news dataset. I will first provide you with the truth labels corresponding to the 4 types of texts in the ag news dataset (in total the dataset has over one hundred thousand different texts then iteratively I will provide you with the current prompts that have been given to the model and the corresponding accuracy of the model per label on the task, I want you to provide better prompts for all the four labels so that the accuracy of the model improves. We will have many iterations of this process and I want you to iteratively give better prompts. Your output should be of the format 0: \\\"prompt0\\\", 1: \\\"prompt1\\\", 2: \\\"prompt2\\\", 3: \\\"prompt3\\\" (without the \\ because that's only for passing double string to you in python) where the values of the dictionary correspond to the prompts that you generate. Do not generate any text other than these prompts in dictionary format. If after 2-3 iterations the accuracy doesn't increase by much than try different methods. Also if you are able to achieve precision of over 90 for any of the four labels then don't change the prompt for those by much.\"\n",
    "#     current_prompt = initial_prompt\n",
    "\n",
    "#     # Iteratively refine the prompt using Gemma2\n",
    "#     iterations = 3\n",
    "#     for i in range(iterations):\n",
    "#         print(f\"Iteration {i + 1}: Current Prompt -> {current_prompt}\")\n",
    "#         refined_prompt = interact_with_gemma2(gemma2, current_prompt)\n",
    "#         print(f\"Refined Prompt -> {refined_prompt}\")\n",
    "#         current_prompt = refined_prompt\n",
    "\n",
    "#         # Use the refined prompt for classification\n",
    "#         print(\"\\nUsing the final refined prompt for analysis...\")\n",
    "#         candidate_labels = [f\"{refined_prompt} ({label})\" for label in [\"politics\", \"sports\", \"economics\", \"science and technology\"]]\n",
    "\n",
    "#         # Perform main error analysis with the refined prompt\n",
    "#         results_df = perform_error_analysis(ds, classifier, num_samples=500, prompt=refined_prompt)\n",
    "        \n",
    "#         # Analyze error patterns\n",
    "#         analyze_error_patterns(results_df)\n",
    "\n",
    "#     # Terminate Gemma2 session\n",
    "#     gemma2.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ollama import chat\n",
    "\n",
    "# models = ['nemotron']\n",
    "\n",
    "message = '''You are an expert prompt generator for a zero shot text classification task. I am using roberta-large-mnli to do zero shot classification on the ag news dataset.  \n",
    "I will first provide you with the truth labels corresponding to the 4 types of texts in the ag news dataset (in total the dataset has over one hundred thousand different texts). \n",
    "The AG News dataset provides us with the following intial labels: 0: World, 1: Sports, 2: Business, 4: Sci/Tech\n",
    "Then iteratively, I will provide you with the current prompts that have been given to the model and the corresponding accuracy of the model per label on the task.\n",
    "I want you to provide better prompts for all the four labels so that the accuracy of the model improves. We will have many iterations of this process and I want you to iteratively give better prompts.\n",
    "I will also give you the per class accuracy, the most confident mistakes, and common error patterns observed for the given labels. Infer whatever you want to from them and try to make your results better.(Follow the format exactly, dont miss any commas or inverted commas)\n",
    "Your output should be of the format 0: \\\"prompt0\\\", 1: \\\"prompt1\\\", 2: \\\"prompt2\\\", 3: \\\"prompt3\\\" where the values of the dictionary correspond to the prompts that you generate. (The prompts aren't questions, they are assertions)\n",
    "Do not generate any text other than these prompts in dictionary format. If after 2-3 iterations the accuracy doesn't increase by much than try different methods. \n",
    "Also if you are able to achieve precision of over 90 for any of the four labels then don't change the prompt for those by much.\n",
    "This is my initial input to you, from the next message onwards, I will start giving the above mentioned data as the message, I won't provide any other information in the subsequent messages, so use the information I gave in this message to interpret the data provided.\n",
    "Also, don't give any non alphanumeric characters in the output such as emojis or emoticons. \n",
    "Overall Accuracy: 48.50%\n",
    "\n",
    "Per-class Performance:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    Business       0.36      0.08      0.13       205\n",
    "    Sci/Tech       0.91      0.21      0.34       253\n",
    "      Sports       0.75      0.68      0.72       274\n",
    "       World       0.35      0.85      0.50       268\n",
    "\n",
    "    accuracy                           0.48      1000\n",
    "   macro avg       0.60      0.46      0.42      1000\n",
    "weighted avg       0.61      0.48      0.44      1000\n",
    "\n",
    "Most Confident Mistakes:\n",
    "                                                                                                                                                                                                                                                                                                    text  \\\n",
    "702                                                                                                                                  HP Unveils Cavalcade of Consumer Products (PC World) PC World - First TVs, new printers, long-lasting inks, and projectors are targeted\\ at living room and office.   \n",
    "624  World #39;s smallest digital camera with zoom lens Come September, Japanese electronics giant Casio Computer will launch the world #39;s smallest digital camera with a zoom lens. Casio #39;s palm-sized Exilim camera is much smaller than others as, for the first time, it uses a ceramic lens.   \n",
    "468                                                                                              Microsoft: Use Script to Block Windows XP SP2 Updates Microsoft has offered up yet another way for businesses to block the automatic update of Windows XP to the big-deal Service Pack 2 (SP2) upgrade.   \n",
    "821                                                                                                  We owe Athens an apology ATHENS -- The Games of the XXVIII Olympiad -- the great disaster that wasn #39;t -- come to an emotional end this afternoon and, really, the world owes Athens an apology.   \n",
    "963                                       EU, Japan Win WTO Approval to Impose Duties on US (Update2) The European Union, Japan and Brazil won World Trade Organization backing to impose tariffs on US imports after Congress failed to end illegal corporate subsidies worth \\$850 million since 2001.   \n",
    "\n",
    "    true_label predicted_label  confidence  \n",
    "702   Sci/Tech           World    0.952798  \n",
    "624   Sci/Tech           World    0.937181  \n",
    "468   Sci/Tech        Business    0.881427  \n",
    "821     Sports           World    0.867683  \n",
    "963   Business           World    0.826846  \n",
    "\n",
    "\n",
    "Common Error Patterns:\n",
    "Business â†’ World: 173\n",
    "Sci/Tech â†’ World: 160\n",
    "Sports â†’ World: 87\n",
    "World â†’ Sports: 36\n",
    "Sci/Tech â†’ Business: 28\n",
    "Business â†’ Sports: 14\n",
    "Sci/Tech â†’ Sports: 12\n",
    "World â†’ Sci/Tech: 3\n",
    "Business â†’ Sci/Tech: 2\n",
    "\n",
    "(print only in the format described and print nothing else, don't forget the double quotations for the prompts)\n",
    "    Output format: {{0: \"prompt0\", 1: \"prompt1\", 2: \"prompt2\", 3: \"prompt3\"}}\n",
    "\n",
    "'''\n",
    "\n",
    "# for i in models:\n",
    "#   print(\"Model name:\", i)\n",
    "#   stream = chat(\n",
    "#       model=i,\n",
    "#       messages=[{'role': 'user', 'content': message}],\n",
    "#       stream=True,\n",
    "#   )\n",
    "  \n",
    "#   for chunk in stream:\n",
    "#     print(chunk['message']['content'], end='', flush=True)\n",
    "#   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: \"This text discusses global events and international relations.\", 1: \"This text focuses on athletic competitions and sporting achievements.\", 2: \"This text reports on financial markets, companies, and economic trends.\", 3: \"This text covers scientific advancements, technological innovations, and the digital world.\"}\n",
      "\n",
      "Iteration 2: Current label prompts:\n",
      "0: \"This text discusses global events and international relations.\"\n",
      "1: \"This text focuses on athletic competitions and sporting achievements.\"\n",
      "2: \"This text reports on financial markets, companies, and economic trends.\"\n",
      "3: \"This text covers scientific advancements, technological innovations, and the digital world.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34fc76088114802aebade7a990626dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MAX_ITERATIONS = 50\n",
    "\n",
    "# Initialize model and variables\n",
    "model_name = 'gemma2'\n",
    "\n",
    "# Initial label map\n",
    "label_map = dict()\n",
    "best_accuracy = 48.5\n",
    "accuracy_data = [48.5]\n",
    "iteration_data = list(range(1, MAX_ITERATIONS + 1))\n",
    "\n",
    "initial_interaction = True\n",
    "# Function to interact with the LLM\n",
    "def interact_with_llm(model_name, prompt):\n",
    "    \n",
    "    stream = chat(\n",
    "        model=model_name,\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content']\n",
    "    new_labels=response.strip()\n",
    "    print(new_labels)\n",
    "    dictionary = ast.literal_eval(new_labels)\n",
    "    return dictionary\n",
    "\n",
    "# Loop for iterative prompt refinement\n",
    "iteration = 1\n",
    "response = \"\"\n",
    "while iteration <= MAX_ITERATIONS:\n",
    "    if(initial_interaction):\n",
    "        label_map = interact_with_llm(model_name, message)\n",
    "        initial_interaction = False\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nIteration {iteration}: Current label prompts:\")\n",
    "        for k, v in label_map.items():\n",
    "            print(f\"{k}: \\\"{v}\\\"\") \n",
    "        \n",
    "\n",
    "        # Get outputs from func1 and func2\n",
    "        results_df, output1, accuracy = perform_error_analysis(ds, classifier, label_map, num_samples=3000)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "        output2 = analyze_error_patterns(results_df)\n",
    "        accuracy_data.append(accuracy)\n",
    "\n",
    "        # print(f\"func1 output: {output1}\")\n",
    "        # print(f\"func2 output: {output2}\")\n",
    "        \n",
    "        # Prepare input message for the LLM\n",
    "        llm_input = f\"\"\"\n",
    "    Here are the current prompts for labels:\\n{label_map}\n",
    "    Here is the analysis for your last set of prompts/labels for the data:\n",
    "    {output1}\n",
    "    {output2}\n",
    "\n",
    "    Using this and the current prompts, generate improved prompts for the labels.(print only in the format described and print nothing else)\n",
    "    Output format: {{0: \"prompt0\", 1: \"prompt1\", 2: \"prompt2\", 3: \"prompt3\"}}\"\"\"\n",
    "    \n",
    "        print(llm_input)\n",
    "\n",
    "        # Send current prompts and function outputs to the LLM\n",
    "        label_map = interact_with_llm(model_name, llm_input)\n",
    "\n",
    "        \n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(iteration_data, accuracy_data, marker= 'o', linestyle = '-', color = 'b', label = 'Data Points')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.title(\"Prediction Accuracy obtained using labels returned by a LLM\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Best Accuracy: \" + best_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article discusses advancements in technology and scientific breakthroughs.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "import ast\n",
    "\n",
    "# Initialize model and variables\n",
    "# model_name = 'nemotron'\n",
    "\n",
    "# # Initial label map\n",
    "# label_map = dict()\n",
    "\n",
    "# initial_interaction = True\n",
    "# Function to interact with the LLM\n",
    "# response = interact_with_llm(model_name, message)\n",
    "input = '''0: \"This article discusses advancements in technology and scientific breakthroughs.\", \n",
    "1: \"This article reports on sporting events, athletes, and competitions.\", \n",
    "2: \"This article covers financial news, market trends, and business developments.\", \n",
    "3: \"This article provides information about global events, politics, and international relations.\"\n",
    "'''\n",
    "\n",
    "formatted_string = \"{\" + input + \"}\"\n",
    "dictionary = ast.literal_eval(formatted_string)\n",
    "print(dictionary[0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
