{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_name = \"roberta-large-mnli\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"wangrongsheng/ag_news\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('zero-shot-classification', model=model_name, device=device)                              \n",
    "sequence_to_classify = \"This is a test\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing', 'technician', 'teacher']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_error_analysis(dataset, classifier, num_samples=100):\n",
    "    \"\"\"\n",
    "    Perform comprehensive error analysis on the zero-shot classifier.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The AG News dataset\n",
    "        classifier: The zero-shot classification pipeline\n",
    "        num_samples: Number of samples to analyze (use smaller number for testing)\n",
    "    \"\"\"\n",
    "    # AG News labels mapping\n",
    "    # label_map = {\n",
    "    #     0: \"This document is about politics\",\n",
    "    #     1: \"This document is about sports\",\n",
    "    #     2: \"This document is about economics\",\n",
    "    #     3: \"This document is about science and technology\"\n",
    "    # }\n",
    "    \n",
    "    label_map = {\n",
    "        0: \"politics\",\n",
    "        1: \"sports\",\n",
    "        2: \"economics\",\n",
    "        3: \"science and technology\"\n",
    "    }\n",
    "    \n",
    "    # Prepare candidate labels for zero-shot classification\n",
    "    candidate_labels = list(label_map.values())\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Process test samples\n",
    "    for i, item in tqdm(enumerate(dataset['test']), total=num_samples):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        text = item['text']\n",
    "        true_label = label_map[item['label']]\n",
    "        \n",
    "        # Get model prediction\n",
    "        prediction = classifier(text, candidate_labels)\n",
    "        predicted_label = prediction['labels'][0]\n",
    "        confidence = prediction['scores'][0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'confidence': confidence,\n",
    "            'correct': true_label == predicted_label\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # 1. Overall Accuracy\n",
    "    accuracy = (df_results['correct'].sum() / len(df_results)) * 100\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # 2. Per-class Performance\n",
    "    print(\"\\nPer-class Performance:\")\n",
    "    print(classification_report(df_results['true_label'], df_results['predicted_label']))\n",
    "    \n",
    "    # 3. Confusion Matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(df_results['true_label'], df_results['predicted_label'], labels=candidate_labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=candidate_labels, yticklabels=candidate_labels)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Confidence Analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='true_label', y='confidence', hue='correct', data=df_results)\n",
    "    plt.title('Confidence Distribution by Class and Correctness')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Error Examples Analysis\n",
    "    print(\"\\nMost Confident Mistakes:\")\n",
    "    mistakes = df_results[~df_results['correct']].sort_values('confidence', ascending=False)\n",
    "    print(mistakes[['text', 'true_label', 'predicted_label', 'confidence']].head())\n",
    "    \n",
    "    # 6. Length Analysis\n",
    "    df_results['text_length'] = df_results['text'].str.len()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='true_label', y='text_length', hue='correct', data=df_results)\n",
    "    plt.title('Text Length Distribution by Class and Correctness')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_patterns(df_results):\n",
    "    \"\"\"\n",
    "    Analyze specific patterns in the errors.\n",
    "    \"\"\"\n",
    "    # Common misclassification patterns\n",
    "    error_patterns = defaultdict(int)\n",
    "    for _, row in df_results[~df_results['correct']].iterrows():\n",
    "        pattern = f\"{row['true_label']} â†’ {row['predicted_label']}\"\n",
    "        error_patterns[pattern] += 1\n",
    "    \n",
    "    print(\"\\nCommon Error Patterns:\")\n",
    "    for pattern, count in sorted(error_patterns.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{pattern}: {count}\")\n",
    "        \n",
    "    # Confidence threshold analysis\n",
    "    thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "    accuracies = []\n",
    "    coverage = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        filtered_preds = df_results[df_results['confidence'] >= threshold]\n",
    "        if len(filtered_preds) > 0:\n",
    "            acc = (filtered_preds['correct'].sum() / len(filtered_preds)) * 100\n",
    "            cov = (len(filtered_preds) / len(df_results)) * 100\n",
    "            accuracies.append(acc)\n",
    "            coverage.append(cov)\n",
    "    \n",
    "    # Ensure arrays are the same length before plotting\n",
    "    min_len = min(len(accuracies), len(coverage))\n",
    "    thresholds = thresholds[:min_len]\n",
    "    accuracies = accuracies[:min_len]\n",
    "    coverage = coverage[:min_len]\n",
    "    \n",
    "    if min_len > 0:  # Only plot if we have data\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(thresholds, accuracies, 'b-', label='Accuracy')\n",
    "        plt.plot(thresholds, coverage, 'r-', label='Coverage')\n",
    "        plt.xlabel('Confidence Threshold')\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.title('Accuracy vs Coverage Trade-off')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Warning: Not enough data points to create accuracy-coverage plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Perform main error analysis\n",
    "    results_df = perform_error_analysis(ds, classifier, num_samples=500)\n",
    "    \n",
    "    # Analyze error patterns\n",
    "    analyze_error_patterns(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
